# ğŸ•µï¸â€â™‚ï¸ Responsible AI Inspector

A short case study collection investigating how artificial intelligence systems can go wrong â€” and how we can design them better. Created as part of a Responsible AI assignment for the PLP program, this repo showcases ethics in action through two fictional scenarios.


## ğŸ” Case Files

### 1ï¸âƒ£ The Hiring Bot with Hidden Bias
- ğŸ“Œ AI rejects candidates with career gaps â€” especially women.
- âš ï¸ Bias from historical data, no transparency, zero accountability.
- âœ… Fix: Add fairness constraints + human review in edge cases.

### 2ï¸âƒ£ The School Proctoring AI with Tunnel Vision
- ğŸ“Œ Flags cheating based on eye movement in remote exams.
- âš ï¸ Neurodivergent students penalized unfairly.
- âœ… Fix: Add accommodations + human-in-the-loop oversight.

## âœï¸ Blog-Style Narratives
Each case is written like a mini blog post â€” accessible, thoughtful, and inspector-themed. Full write-up in `CASE_NOTES.md`.

## ğŸ‘¤ Author
**Leonard Phokane**  
Ethical AI Advocate | Creative Technologist  
ğŸ”— [GitHub Portfolio](https://github.com/leonardphokane)  
ğŸŒ [Portfolio Site](https://phokane-creative-code.lovable.app/)

## âœ¨ Use This Format
Got your own AI ethics story? Fork this repo and submit your case â€” let's make bias visible and accountability non-negotiable.
