# 🕵️‍♂️ Responsible AI Inspector

A short case study collection investigating how artificial intelligence systems can go wrong — and how we can design them better. Created as part of a Responsible AI assignment for the PLP program, this repo showcases ethics in action through two fictional scenarios.


## 🔍 Case Files

### 1️⃣ The Hiring Bot with Hidden Bias
- 📌 AI rejects candidates with career gaps — especially women.
- ⚠️ Bias from historical data, no transparency, zero accountability.
- ✅ Fix: Add fairness constraints + human review in edge cases.

### 2️⃣ The School Proctoring AI with Tunnel Vision
- 📌 Flags cheating based on eye movement in remote exams.
- ⚠️ Neurodivergent students penalized unfairly.
- ✅ Fix: Add accommodations + human-in-the-loop oversight.

## ✍️ Blog-Style Narratives
Each case is written like a mini blog post — accessible, thoughtful, and inspector-themed. Full write-up in `CASE_NOTES.md`.

## 👤 Author
**Leonard Phokane**  
Ethical AI Advocate | Creative Technologist  
🔗 [GitHub Portfolio](https://github.com/leonardphokane)  
🌐 [Portfolio Site](https://phokane-creative-code.lovable.app/)

## ✨ Use This Format
Got your own AI ethics story? Fork this repo and submit your case — let's make bias visible and accountability non-negotiable.
